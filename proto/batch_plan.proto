syntax = "proto3";

package batch_plan;

import "common.proto";
import "data.proto";
import "expr.proto";
import "plan_common.proto";

option optimize_for = SPEED;

message RowSeqScanNode {
  plan_common.CellBasedTableDesc table_desc = 1;
  repeated plan_common.ColumnDesc column_descs = 2;
}

message SourceScanNode {
  plan_common.TableRefId table_ref_id = 1;
  // timestamp_ms is used for offset synchronization of high level consumer groups, this field will be deprecated if a more elegant approach is available in the future
  int64 timestamp_ms = 2;
  repeated int32 column_ids = 3;
}

message ProjectNode {
  repeated expr.ExprNode select_list = 1;
}

message FilterNode {
  expr.ExprNode search_condition = 1;
}

message FilterScanNode {
  plan_common.TableRefId table_ref_id = 1;
  repeated int32 column_ids = 2;
}

message InsertNode {
  plan_common.TableRefId table_source_ref_id = 1;
  repeated int32 column_ids = 2;
}

message DeleteNode {
  plan_common.TableRefId table_source_ref_id = 1;
}

message UpdateNode {
  plan_common.TableRefId table_source_ref_id = 1;
  repeated expr.ExprNode exprs = 2;
}

message ValuesNode {
  message ExprTuple {
    repeated expr.ExprNode cells = 1;
  }
  repeated ExprTuple tuples = 1;
  repeated plan_common.Field fields = 2;
}

message OrderByNode {
  repeated plan_common.ColumnOrder column_orders = 1;
}

message TopNNode {
  repeated plan_common.ColumnOrder column_orders = 1;
  uint32 limit = 2;
  uint32 offset = 3;
}

message LimitNode {
  uint32 limit = 1;
  uint32 offset = 2;
}

message NestedLoopJoinNode {
  plan_common.JoinType join_type = 1;
  expr.ExprNode join_cond = 2;
}

message HashAggNode {
  repeated uint32 group_keys = 1;
  repeated expr.AggCall agg_calls = 2;
}

message SortAggNode {
  repeated expr.ExprNode group_keys = 1;
  repeated expr.AggCall agg_calls = 2;
}

message HashJoinNode {
  plan_common.JoinType join_type = 1;
  repeated int32 left_key = 2;
  repeated int32 right_key = 3;
  expr.ExprNode condition = 4;
}

message SortMergeJoinNode {
  plan_common.JoinType join_type = 1;
  repeated int32 left_keys = 2;
  repeated int32 right_keys = 3;
  plan_common.OrderType direction = 4;
}

message HopWindowNode {
  expr.InputRefExpr time_col = 1;
  data.IntervalUnit window_slide = 2;
  data.IntervalUnit window_size = 3;
}

message GenerateSeriesNode {
  expr.ExprNode start = 1;
  expr.ExprNode stop = 2;
  expr.ExprNode step = 3;
}

// Task is a running instance of Stage.
message TaskId {
  string query_id = 1;
  uint32 stage_id = 2;
  uint32 task_id = 3;
}

// Every task will create N buffers (channels) for parent operators to fetch results from,
// where N is the parallelism of parent stage.
message TaskOutputId {
  TaskId task_id = 1;
  // The id of output channel to fetch from
  uint32 output_id = 2;
}

// ExchangeSource describes where to read results from children operators
message ExchangeSource {
  TaskOutputId task_output_id = 1;
  common.HostAddress host = 2;
}

message ExchangeNode {
  repeated ExchangeSource sources = 1;
  repeated plan_common.Field input_schema = 3;
}

message MergeSortExchangeNode {
  ExchangeNode exchange = 1;
  repeated plan_common.ColumnOrder column_orders = 2;
}

message PlanNode {
  repeated PlanNode children = 1;
  oneof node_body {
    InsertNode insert = 2;
    DeleteNode delete = 3;
    UpdateNode update = 4;
    ProjectNode project = 5;
    HashAggNode hash_agg = 7;
    FilterNode filter = 8;
    ExchangeNode exchange = 9;
    OrderByNode order_by = 10;
    NestedLoopJoinNode nested_loop_join = 11;
    TopNNode top_n = 14;
    SortAggNode sort_agg = 15;
    RowSeqScanNode row_seq_scan = 16;
    LimitNode limit = 17;
    ValuesNode values = 18;
    HashJoinNode hash_join = 19;
    MergeSortExchangeNode merge_sort_exchange = 21;
    SortMergeJoinNode sort_merge_join = 22;
    HopWindowNode hop_window = 25;
    GenerateSeriesNode generate_series = 26;
  }
  string identity = 24;
}

// ExchangeInfo determines how to distribute results to tasks of next stage.
//
// Note that the fragment itself does not know the where are the receivers. Instead, it prepares results in
// N buffers and wait for parent operators (`Exchange` nodes) to pull data from a specified buffer
message ExchangeInfo {
  enum DistributionMode {
    // No partitioning at all, used for root segment which aggregates query results
    SINGLE = 0;
    BROADCAST = 1;
    HASH = 2;
  }
  message BroadcastInfo {
    uint32 count = 1;
  }
  message HashInfo {
    uint32 output_count = 1;
    repeated uint32 keys = 3;
  }
  DistributionMode mode = 1;
  oneof distribution {
    BroadcastInfo broadcast_info = 2;
    HashInfo hash_info = 3;
  }
}

message PlanFragment {
  PlanNode root = 1;
  ExchangeInfo exchange_info = 2;
}
