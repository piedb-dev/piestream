# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- id: create_tables
  sql: |
    CREATE TABLE person (
        id BIGINT,
        name VARCHAR,
        emailAddress VARCHAR,
        creditCard VARCHAR,
        city VARCHAR,
        state VARCHAR,
        dateTime TIMESTAMP
    );

    CREATE TABLE auction (
        id BIGINT,
        itemName VARCHAR,
        description VARCHAR,
        initialBid BIGINT,
        reserve BIGINT,
        dateTime TIMESTAMP,
        expires TIMESTAMP,
        seller BIGINT,
        category BIGINT
    );

    CREATE TABLE bid (
        auction BIGINT,
        bidder BIGINT,
        price BIGINT,
        channel VARCHAR,
        url VARCHAR,
        dateTime TIMESTAMP
    );
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    CREATE MATERIALIZED VIEW nexmark_q0
      AS
    SELECT auction, bidder, price, dateTime FROM bid;
- id: nexmark_q0
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, dateTime FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, dateTime, _row_id(hidden)], pk_columns: [_row_id] }
      StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q1
  before:
    - create_tables
  sql: |
    SELECT
      auction,
      bidder,
      0.908 * price as price,
      dateTime
    FROM bid;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1, (0.908:Decimal * $2), $3] }
        BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, bidder, price, dateTime, _row_id(hidden)], pk_columns: [_row_id] }
      StreamProject { exprs: [$0, $1, (0.908:Decimal * $2), $3, $4] }
        StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
- id: nexmark_q2
  before:
    - create_tables
  sql: "SELECT auction, price FROM bid \nWHERE auction = 1007 OR auction = 1020 OR auction = 2001 OR auction = 2019 OR auction = 2087;\n"
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        BatchScan { table: bid, columns: [auction, price] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, _row_id(hidden)], pk_columns: [_row_id] }
      StreamFilter { predicate: ((((($0 = 1007:Int32) OR ($0 = 1020:Int32)) OR ($0 = 2001:Int32)) OR ($0 = 2019:Int32)) OR ($0 = 2087:Int32)) }
        StreamTableScan { table: bid, columns: [auction, price, _row_id], pk_indices: [2] }
- id: nexmark_q3
  before:
    - create_tables
  sql: |
    SELECT
        P.name, P.city, P.state, A.id
    FROM
        auction AS A INNER JOIN person AS P on A.seller = P.id
    WHERE
        A.category = 10 and (P.state = 'or' OR P.state = 'id' OR P.state = 'ca');
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$1, $2, $3, $0] }
        BatchHashJoin { type: Inner, predicate: $1 = $2, output_indices: [0, 3, 4, 5] }
          BatchExchange { order: [], dist: HashShard([1]) }
            BatchProject { exprs: [$0, $1] }
              BatchFilter { predicate: ($2 = 10:Int32) }
                BatchScan { table: auction, columns: [id, seller, category] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
              BatchScan { table: person, columns: [id, name, city, state] }
  stream_plan: |
    StreamMaterialize { columns: [name, city, state, id, _row_id(hidden), _row_id#1(hidden)], pk_columns: [_row_id, _row_id#1] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$1, $2, $3, $0, $4, $5] }
          StreamHashJoin { type: Inner, predicate: $1 = $3, output_indices: [0, 4, 5, 6, 2, 7] }
            StreamExchange { dist: HashShard([1]) }
              StreamProject { exprs: [$0, $1, $2] }
                StreamFilter { predicate: ($3 = 10:Int32) }
                  StreamTableScan { table: auction, columns: [id, seller, _row_id, category], pk_indices: [2] }
            StreamExchange { dist: HashShard([0]) }
              StreamFilter { predicate: ((($3 = 'or':Varchar) OR ($3 = 'id':Varchar)) OR ($3 = 'ca':Varchar)) }
                StreamTableScan { table: person, columns: [id, name, city, state, _row_id], pk_indices: [4] }
- id: nexmark_q4
  before:
    - create_tables
  sql: |
    SELECT
        Q.category,
        AVG(Q.final) as avg
    FROM (
        SELECT MAX(B.price) AS final, A.category
        FROM auction A, bid B
        WHERE A.id = B.auction AND B.dateTime BETWEEN A.dateTime AND A.expires
        GROUP BY A.id, A.category
    ) Q
    GROUP BY Q.category;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, ($1 / $2)] }
        BatchHashAgg { group_keys: [$0], aggs: [sum($1), count($1)] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchProject { exprs: [$1, $2] }
              BatchHashAgg { group_keys: [$0, $1], aggs: [max($2)] }
                BatchProject { exprs: [$0, $3, $5] }
                  BatchFilter { predicate: ($6 >= $1) AND ($6 <= $2) }
                    BatchHashJoin { type: Inner, predicate: $0 = $4, output_indices: all }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: auction, columns: [id, dateTime, expires, category] }
                      BatchExchange { order: [], dist: HashShard([0]) }
                        BatchScan { table: bid, columns: [auction, price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [category, avg], pk_columns: [category] }
      StreamProject { exprs: [$0, ($2 / $3)] }
        StreamHashAgg { group_keys: [$0], aggs: [count, sum($1), count($1)] }
          StreamExchange { dist: HashShard([0]) }
            StreamProject { exprs: [$1, $3, $0] }
              StreamHashAgg { group_keys: [$0, $1], aggs: [count, max($2)] }
                StreamProject { exprs: [$0, $3, $6, $4, $8] }
                  StreamFilter { predicate: ($7 >= $1) AND ($7 <= $2) }
                    StreamHashJoin { type: Inner, predicate: $0 = $5, output_indices: all }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: auction, columns: [id, dateTime, expires, category, _row_id], pk_indices: [4] }
                      StreamExchange { dist: HashShard([0]) }
                        StreamTableScan { table: bid, columns: [auction, price, dateTime, _row_id], pk_indices: [3] }
- id: nexmark_q5
  before:
    - create_tables
  sql: "SELECT AuctionBids.auction, AuctionBids.num\nFROM (\n    SELECT\n        bid.auction,\n        count(*) AS num, \n        window_start AS starttime\n    FROM \n        HOP(bid, dateTime, INTERVAL '2' SECOND, INTERVAL '10' SECOND)\n    GROUP BY\n        window_start, \n    bid.auction\n) AS AuctionBids\nJOIN (\n    SELECT\n        max(CountBids.num) AS maxn,\n        CountBids.starttime_c\n    FROM (\n        SELECT\n            count(*) AS num,\n            window_start AS starttime_c\n        FROM HOP(bid, dateTime, INTERVAL '2' SECOND, INTERVAL '10' SECOND)\n        GROUP BY\n            bid.auction,\n            window_start\n        ) AS CountBids\n    GROUP BY \n        CountBids.starttime_c\n    ) AS MaxBids\nON \n  AuctionBids.starttime = MaxBids.starttime_c AND\n  AuctionBids.num >= MaxBids.maxn;\n"
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $1] }
        BatchFilter { predicate: ($1 >= $3) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchProject { exprs: [$1, $2, $0] }
                BatchHashAgg { group_keys: [$0, $1], aggs: [count] }
                  BatchExchange { order: [], dist: HashShard([0, 1]) }
                    BatchProject { exprs: [$1, $0] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchScan { table: bid, columns: [auction, dateTime] }
            BatchProject { exprs: [$1, $0] }
              BatchHashAgg { group_keys: [$0], aggs: [max($1)] }
                BatchExchange { order: [], dist: HashShard([0]) }
                  BatchProject { exprs: [$1, $2] }
                    BatchHashAgg { group_keys: [$0, $1], aggs: [count] }
                      BatchHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 2] }
                        BatchExchange { order: [], dist: HashShard([0]) }
                          BatchScan { table: bid, columns: [auction, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, num, window_start(hidden), window_start#1(hidden)], pk_columns: [window_start, auction, window_start#1] }
      StreamProject { exprs: [$0, $1, $2, $4] }
        StreamFilter { predicate: ($1 >= $3) }
          StreamHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            StreamExchange { dist: HashShard([2]) }
              StreamProject { exprs: [$1, $3, $0] }
                StreamHashAgg { group_keys: [$0, $1], aggs: [count, count] }
                  StreamExchange { dist: HashShard([0, 1]) }
                    StreamProject { exprs: [$1, $0, $2] }
                      StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                        StreamTableScan { table: bid, columns: [auction, dateTime, _row_id], pk_indices: [2] }
            StreamProject { exprs: [$2, $0] }
              StreamHashAgg { group_keys: [$0], aggs: [count, max($1)] }
                StreamExchange { dist: HashShard([0]) }
                  StreamProject { exprs: [$1, $3, $0] }
                    StreamHashAgg { group_keys: [$0, $1], aggs: [count, count] }
                      StreamExchange { dist: HashShard([0, 1]) }
                        StreamHopWindow { time_col: $1, slide: 00:00:02, size: 00:00:10, output_indices: [0, 3, 2] }
                          StreamTableScan { table: bid, columns: [auction, dateTime, _row_id], pk_indices: [2] }
- id: nexmark_q6
  before:
    - create_tables
  sql: |
    SELECT
        Q.seller,
        AVG(Q.final) OVER
            (PARTITION BY Q.seller ORDER BY Q.dateTime ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)
        as avg
    FROM (
        SELECT MAX(B.price) AS final, A.seller, B.dateTime
        FROM auction AS A, bid AS B
        WHERE A.id = B.auction and B.dateTime between A.dateTime and A.expires
        GROUP BY A.id, A.seller
    ) AS Q;
  planner_error: 'Invalid input syntax: column must appear in the GROUP BY clause or be used in an aggregate function'
- id: nexmark_q7
  before:
    - create_tables
  sql: |
    SELECT
        B.auction,
        B.price,
        B.bidder,
        B.dateTime
    from
        bid B
        JOIN (
            SELECT
                MAX(price) AS maxprice,
                window_end as dateTime
            FROM
                TUMBLE(bid, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                window_end
        ) B1 ON B.price = B1.maxprice
    WHERE
        B.dateTime BETWEEN B1.dateTime - INTERVAL '10' SECOND
        AND B1.dateTime;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$0, $2, $1, $3] }
        BatchFilter { predicate: ($3 >= ($5 - '00:00:10':Interval)) AND ($3 <= $5) }
          BatchHashJoin { type: Inner, predicate: $2 = $4, output_indices: all }
            BatchExchange { order: [], dist: HashShard([2]) }
              BatchScan { table: bid, columns: [auction, bidder, price, dateTime] }
            BatchExchange { order: [], dist: HashShard([0]) }
              BatchProject { exprs: [$1, $0] }
                BatchHashAgg { group_keys: [$0], aggs: [max($1)] }
                  BatchExchange { order: [], dist: HashShard([0]) }
                    BatchProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0] }
                      BatchScan { table: bid, columns: [price, dateTime] }
  stream_plan: |
    StreamMaterialize { columns: [auction, price, bidder, dateTime, _row_id(hidden), expr#0(hidden)], pk_columns: [_row_id, expr#0] }
      StreamExchange { dist: HashShard([4, 5]) }
        StreamProject { exprs: [$0, $2, $1, $3, $4, $6] }
          StreamFilter { predicate: ($3 >= ($6 - '00:00:10':Interval)) AND ($3 <= $6) }
            StreamHashJoin { type: Inner, predicate: $2 = $5, output_indices: all }
              StreamExchange { dist: HashShard([2]) }
                StreamTableScan { table: bid, columns: [auction, bidder, price, dateTime, _row_id], pk_indices: [4] }
              StreamExchange { dist: HashShard([0]) }
                StreamProject { exprs: [$2, $0] }
                  StreamHashAgg { group_keys: [$0], aggs: [count, max($1)] }
                    StreamExchange { dist: HashShard([0]) }
                      StreamProject { exprs: [(TumbleStart($1, '00:00:10':Interval) + '00:00:10':Interval), $0, $2] }
                        StreamTableScan { table: bid, columns: [price, dateTime, _row_id], pk_indices: [2] }
- id: nexmark_q8
  before:
    - create_tables
  sql: |
    SELECT
        P.id,
        P.name,
        P.starttime
    FROM
        (
            SELECT
                id,
                name,
                window_start AS starttime,
                window_end AS endtime
            FROM
                TUMBLE(person, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                id,
                name,
                window_start,
                window_end
        ) P
        JOIN (
            SELECT
                seller,
                window_start AS starttime,
                window_end AS endtime
            FROM
                TUMBLE(auction, dateTime, INTERVAL '10' SECOND)
            GROUP BY
                seller,
                window_start,
                window_end
        ) A ON P.id = A.seller
        AND P.starttime = A.starttime
        AND P.endtime = A.endtime;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchHashJoin { type: Inner, predicate: $0 = $4 AND $2 = $5 AND $3 = $6, output_indices: [0, 1, 2] }
        BatchExchange { order: [], dist: HashShard([0, 2, 3]) }
          BatchHashAgg { group_keys: [$0, $1, $2, $3], aggs: [] }
            BatchExchange { order: [], dist: HashShard([0, 1, 2, 3]) }
              BatchProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval)] }
                BatchScan { table: person, columns: [id, name, dateTime] }
        BatchHashAgg { group_keys: [$0, $1, $2], aggs: [] }
          BatchExchange { order: [], dist: HashShard([0, 1, 2]) }
            BatchProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval)] }
              BatchScan { table: auction, columns: [dateTime, seller] }
  stream_plan: |
    StreamMaterialize { columns: [id, name, starttime, expr#3(hidden), seller(hidden), expr#1(hidden), expr#2(hidden)], pk_columns: [id, name, starttime, expr#3, seller, expr#1, expr#2] }
      StreamHashJoin { type: Inner, predicate: $0 = $5 AND $2 = $6 AND $3 = $7, output_indices: [0, 1, 2, 3, 5, 6, 7] }
        StreamExchange { dist: HashShard([0, 2, 3]) }
          StreamHashAgg { group_keys: [$0, $1, $2, $3], aggs: [count] }
            StreamExchange { dist: HashShard([0, 1, 2, 3]) }
              StreamProject { exprs: [$0, $1, TumbleStart($2, '00:00:10':Interval), (TumbleStart($2, '00:00:10':Interval) + '00:00:10':Interval), $3] }
                StreamTableScan { table: person, columns: [id, name, dateTime, _row_id], pk_indices: [3] }
        StreamHashAgg { group_keys: [$0, $1, $2], aggs: [count] }
          StreamExchange { dist: HashShard([0, 1, 2]) }
            StreamProject { exprs: [$1, TumbleStart($0, '00:00:10':Interval), (TumbleStart($0, '00:00:10':Interval) + '00:00:10':Interval), $2] }
              StreamTableScan { table: auction, columns: [dateTime, seller, _row_id], pk_indices: [2] }
- id: nexmark_q9
  before:
    - create_tables
  sql: |
    SELECT
        id, itemName, description, initialBid, reserve, dateTime, expires, seller, category,
        auction, bidder, price, bid_dateTime
    FROM (
      SELECT A.*, B.auction, B.bidder, B.price, B.dateTime AS bid_dateTime,
        ROW_NUMBER() OVER (PARTITION BY A.id ORDER BY B.price DESC, B.dateTime ASC) AS rownum
      FROM auction A, bid B
      WHERE A.id = B.auction AND B.dateTime BETWEEN A.dateTime AND A.expires
    )
    WHERE rownum <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q10
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, dateTime, DATE_FORMAT(dateTime, 'yyyy-MM-dd'), DATE_FORMAT(dateTime, 'HH:mm')
    FROM bid;
  binder_error: 'Feature is not yet implemented: unsupported function: "date_format", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q11
  before:
    - create_tables
  sql: |
    SELECT
        B.bidder,
        count(*) as bid_count,
        SESSION_START(B.dateTime, INTERVAL '10' SECOND) as starttime,
        SESSION_END(B.dateTime, INTERVAL '10' SECOND) as endtime
    FROM bid B
    GROUP BY B.bidder, SESSION(B.dateTime, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "session", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q12
  before:
    - create_tables
  sql: |
    SELECT
        B.bidder,
        count(*) as bid_count,
        TUMBLE_START(B.p_time, INTERVAL '10' SECOND) as starttime,
        TUMBLE_END(B.p_time, INTERVAL '10' SECOND) as endtime
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    GROUP BY B.bidder, TUMBLE(B.p_time, INTERVAL '10' SECOND);
  binder_error: 'Feature is not yet implemented: unsupported function: "proctime", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q13
  before:
    - create_tables
  sql: |
    /* SELECT
        B.auction,
        B.bidder,
        B.price,
        B.dateTime,
        S.`value`
    FROM (SELECT *, PROCTIME() as p_time FROM bid) B
    JOIN side_input FOR SYSTEM_TIME AS OF B.p_time AS S
    ON mod(B.auction, 10000) = S.key; */
    /* parser error */
    select 1;
- id: nexmark_q14
  before:
    - create_tables
  sql: "SELECT \n    auction,\n    bidder,\n    0.908 * price as price,\n    CASE\n        WHEN HOUR(dateTime) >= 8 AND HOUR(dateTime) <= 18 THEN 'dayTime'\n        WHEN HOUR(dateTime) <= 6 OR HOUR(dateTime) >= 20 THEN 'nightTime'\n        ELSE 'otherTime'\n    END AS bidTimeType,\n    dateTime,\n    extra,\n    count_char(extra, 'c') AS c_counts\nFROM bid\nWHERE 0.908 * price > 1000000 AND 0.908 * price < 50000000;\n"
  binder_error: 'Feature is not yet implemented: unsupported function: "hour", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q15
  before:
    - create_tables
  sql: |
    /* SELECT
        DATE_FORMAT(dateTime, 'yyyy-MM-dd') as `day`,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        count(distinct bidder) AS total_bidders,
        count(distinct bidder) filter (where price < 10000) AS rank1_bidders,
        count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,
        count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,
        count(distinct auction) AS total_auctions,
        count(distinct auction) filter (where price < 10000) AS rank1_auctions,
        count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,
        count(distinct auction) filter (where price >= 1000000) AS rank3_auctions
    FROM bid
    GROUP BY DATE_FORMAT(dateTime, 'yyyy-MM-dd'); */
    /* parser error */
    select 1
- id: nexmark_q16
  before:
    - create_tables
  sql: "/* \nSELECT\n    channel,\n    DATE_FORMAT(dateTime, 'yyyy-MM-dd') as `day`,\n    max(DATE_FORMAT(dateTime, 'HH:mm')) as `minute`,\n    count(*) AS total_bids,\n    count(*) filter (where price < 10000) AS rank1_bids,\n    count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,\n    count(*) filter (where price >= 1000000) AS rank3_bids,\n    count(distinct bidder) AS total_bidders,\n    count(distinct bidder) filter (where price < 10000) AS rank1_bidders,\n    count(distinct bidder) filter (where price >= 10000 and price < 1000000) AS rank2_bidders,\n    count(distinct bidder) filter (where price >= 1000000) AS rank3_bidders,\n    count(distinct auction) AS total_auctions,\n    count(distinct auction) filter (where price < 10000) AS rank1_auctions,\n    count(distinct auction) filter (where price >= 10000 and price < 1000000) AS rank2_auctions,\n    count(distinct auction) filter (where price >= 1000000) AS rank3_auctions\nFROM bid\nGROUP BY channel, DATE_FORMAT(dateTime, 'yyyy-MM-dd'); */\n/* parser error */\nselect 1\n"
- id: nexmark_q17
  before:
    - create_tables
  sql: |
    /*
    SELECT
        auction,
        DATE_FORMAT(dateTime, 'yyyy-MM-dd') as `day`,
        count(*) AS total_bids,
        count(*) filter (where price < 10000) AS rank1_bids,
        count(*) filter (where price >= 10000 and price < 1000000) AS rank2_bids,
        count(*) filter (where price >= 1000000) AS rank3_bids,
        min(price) AS min_price,
        max(price) AS max_price,
        avg(price) AS avg_price,
        sum(price) AS sum_price
    FROM bid
    GROUP BY auction, DATE_FORMAT(dateTime, 'yyyy-MM-dd'); */
    /* parser error */
    select 1
- id: nexmark_q18
  before:
    - create_tables
  sql: |
    SELECT auction, bidder, price, channel, url, dateTime, extra
    FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY bidder, auction ORDER BY dateTime DESC) AS rank_number
          FROM bid)
    WHERE rank_number <= 1;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q19
  before:
    - create_tables
  sql: |
    SELECT * FROM
    (SELECT *, ROW_NUMBER() OVER (PARTITION BY auction ORDER BY price DESC) AS rank_number FROM bid)
    WHERE rank_number <= 10;
  binder_error: 'Feature is not yet implemented: unsupported function: "row_number", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q20
  before:
    - create_tables
  sql: |
    /*
    SELECT
        auction, bidder, price, channel, url, B.dateTime,
        itemName, description, initialBid, reserve, A.dateTime, expires, seller, category,
    FROM
        bid AS B INNER JOIN auction AS A on B.auction = A.id
    WHERE A.category = 10;
    */
    /* parser error */
    select 1
- id: nexmark_q21
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        CASE
            WHEN lower(channel) = 'apple' THEN '0'
            WHEN lower(channel) = 'google' THEN '1'
            WHEN lower(channel) = 'facebook' THEN '2'
            WHEN lower(channel) = 'baidu' THEN '3'
            ELSE REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2)
            END
        AS channel_id FROM bid
        where REGEXP_EXTRACT(url, '(&|^)channel_id=([^&]*)', 2) is not null or
              lower(channel) in ('apple', 'google', 'facebook', 'baidu');
  binder_error: 'Feature is not yet implemented: unsupported function: "regexp_extract", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
- id: nexmark_q22
  before:
    - create_tables
  sql: |
    SELECT
        auction, bidder, price, channel,
        SPLIT_INDEX(url, '/', 3) as dir1,
        SPLIT_INDEX(url, '/', 4) as dir2,
        SPLIT_INDEX(url, '/', 5) as dir3 FROM bid;
  binder_error: 'Feature is not yet implemented: unsupported function: "split_index", Tracking issue: https://github.com/singularity-data/piestream/issues/112'
