# This file is automatically generated. See `src/frontend/test_runner/README.md` for more information.
- sql: |
    create table t(a int, b int);
    select
      max(num) as max_num, a
    from (
      select
        count(*) as num, a, b
      from t
      group by a, b
    )
    group by a;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$1, $0] }
        BatchHashAgg { group_keys: [$0], aggs: [max($1)] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchProject { exprs: [$0, $2] }
              BatchHashAgg { group_keys: [$0, $1], aggs: [count] }
                BatchExchange { order: [], dist: HashShard([0, 1]) }
                  BatchScan { table: t, columns: [a, b] }
  stream_plan: |
    StreamMaterialize { columns: [max_num, a], pk_columns: [a] }
      StreamProject { exprs: [$2, $0] }
        StreamHashAgg { group_keys: [$0], aggs: [count, max($1)] }
          StreamExchange { dist: HashShard([0]) }
            StreamProject { exprs: [$0, $3, $1] }
              StreamHashAgg { group_keys: [$0, $1], aggs: [count, count] }
                StreamExchange { dist: HashShard([0, 1]) }
                  StreamTableScan { table: t, columns: [a, b, _row_id], pk_indices: [2] }
- sql: |
    create table t(a int, b int);
    select
      max(a) as max_a
    from (
      select
        a, a + b as ab
      from t
    )
    group by ab;
  batch_plan: |
    BatchExchange { order: [], dist: Single }
      BatchProject { exprs: [$1] }
        BatchHashAgg { group_keys: [$0], aggs: [max($1)] }
          BatchExchange { order: [], dist: HashShard([0]) }
            BatchProject { exprs: [($0 + $1), $0] }
              BatchScan { table: t, columns: [a, b] }
  stream_plan: |
    StreamMaterialize { columns: [max_a, expr#0(hidden)], pk_columns: [expr#0] }
      StreamProject { exprs: [$2, $0] }
        StreamHashAgg { group_keys: [$0], aggs: [count, max($1)] }
          StreamExchange { dist: HashShard([0]) }
            StreamProject { exprs: [($0 + $1), $0, $2] }
              StreamTableScan { table: t, columns: [a, b, _row_id], pk_indices: [2] }
- sql: |
    create table t1 (row_id int, uid int, v int, created_at timestamp);
    select * from hop(t1, created_at, interval '15' minute, interval '30' minute);
  logical_plan: |
    LogicalProject { exprs: [$1, $2, $3, $4, $5, $6] }
      LogicalHopWindow { time_col: $4, slide: 00:15:00, size: 00:30:00, output_indices: all }
        LogicalScan { table: t1, columns: [_row_id, row_id, uid, v, created_at] }
  optimized_logical_plan: |
    LogicalHopWindow { time_col: $3, slide: 00:15:00, size: 00:30:00, output_indices: all }
      LogicalScan { table: t1, columns: [row_id, uid, v, created_at] }
  batch_plan: |
    BatchHopWindow { time_col: $3, slide: 00:15:00, size: 00:30:00, output_indices: all }
      BatchExchange { order: [], dist: Single }
        BatchScan { table: t1, columns: [row_id, uid, v, created_at] }
  stream_plan: |
    StreamMaterialize { columns: [row_id, uid, v, created_at, window_start, window_end, _row_id(hidden)], pk_columns: [_row_id, window_start] }
      StreamHopWindow { time_col: $3, slide: 00:15:00, size: 00:30:00, output_indices: [0, 1, 2, 3, 5, 6, 4] }
        StreamTableScan { table: t1, columns: [row_id, uid, v, created_at, _row_id], pk_indices: [4] }
